{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "939af857-f09d-4801-81e2-d16adce03f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression:\n",
      "R2 Score: 0.26\n",
      "Mean Squared Error: 3205508802.98\n",
      "Mean Absolute Error: 41855.02\n",
      "\n",
      "Decision Tree Regression:\n",
      "R2 Score: 0.24\n",
      "Mean Squared Error: 3307088252.49\n",
      "Mean Absolute Error: 42941.29\n",
      "\n",
      "KNN Regression:\n",
      "R2 Score: 0.17\n",
      "Mean Squared Error: 3620813676.50\n",
      "Mean Absolute Error: 45072.07\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Load the data\n",
    "file_path = '/Users/merryxmm7/Documents/GitHub/ML_Project_Fashion_Trends/datascience_salaries.csv'\n",
    "df_main = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df_main.drop(['salary', 'salary_currency', 'work_year'], axis=1, inplace=True)\n",
    "#Convert the new dataframe to only US located companies\n",
    "df_main = df_main[df_main['company_location'] == 'US']\n",
    "# Define a function to perform one-hot encoding for categorical features\n",
    "def one_hot_encode(df):\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    transformer = ColumnTransformer([('one_hot_encoder', OneHotEncoder(), categorical_cols)], remainder='passthrough')\n",
    "    return transformer.fit_transform(df)\n",
    "\n",
    "# Perform one-hot encoding for categorical features\n",
    "X = one_hot_encode(df_main.drop('salary_in_usd', axis=1))\n",
    "y = df_main['salary_in_usd']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train Linear Regression model\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train, y_train)\n",
    "\n",
    "# Initialize and train Decision Tree Regression model\n",
    "decision_tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "decision_tree_reg.fit(X_train, y_train)\n",
    "\n",
    "# Initialize and train KNN Regression model\n",
    "knn_reg = KNeighborsRegressor()\n",
    "knn_reg.fit(X_train, y_train)\n",
    "\n",
    "# Function to evaluate models and print metrics\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print(f\"R2 Score: {r2:.2f}\")\n",
    "    print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "    print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "\n",
    "# Evaluate Linear Regression model\n",
    "print(\"Linear Regression:\")\n",
    "evaluate_model(linear_reg, X_test, y_test)\n",
    "print()\n",
    "\n",
    "# Evaluate Decision Tree Regression model\n",
    "print(\"Decision Tree Regression:\")\n",
    "evaluate_model(decision_tree_reg, X_test, y_test)\n",
    "print()\n",
    "\n",
    "# Evaluate KNN Regression model\n",
    "print(\"KNN Regression:\")\n",
    "evaluate_model(knn_reg, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acbed26f-4b69-46dd-86c1-d299e78c2601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Regressor:\n",
      "R2 Score: 0.25\n",
      "Mean Squared Error: 3260406125.17\n",
      "Mean Absolute Error: 42644.79\n",
      "\n",
      "AdaBoost Regressor:\n",
      "R2 Score: 0.15\n",
      "Mean Squared Error: 3697994008.91\n",
      "Mean Absolute Error: 46114.39\n",
      "\n",
      "Gradient Boosting Regressor:\n",
      "R2 Score: 0.26\n",
      "Mean Squared Error: 3232991008.40\n",
      "Mean Absolute Error: 42170.73\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.ensemble import BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler(with_mean=False)  # Set with_mean=False for sparse matrices\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Imbalanced Data Handling (if necessary)\n",
    "# ros = RandomOverSampler(random_state=42)\n",
    "# X_train_resampled, y_train_resampled = ros.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Initialize and train Bagging Regressor model\n",
    "bagging_reg = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=100, random_state=42)\n",
    "bagging_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Initialize and train AdaBoost Regressor model\n",
    "adaboost_reg = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=100, random_state=42)\n",
    "adaboost_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Initialize and train Gradient Boosting Regressor model\n",
    "gradientboost_reg = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gradientboost_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate models\n",
    "print(\"Bagging Regressor:\")\n",
    "evaluate_model(bagging_reg, X_test_scaled, y_test)\n",
    "print()\n",
    "\n",
    "print(\"AdaBoost Regressor:\")\n",
    "evaluate_model(adaboost_reg, X_test_scaled, y_test)\n",
    "print()\n",
    "\n",
    "print(\"Gradient Boosting Regressor:\")\n",
    "evaluate_model(gradientboost_reg, X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9c30338-c2ad-4e63-8e6d-588ab7c91909",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 45\u001b[0m\n\u001b[1;32m     37\u001b[0m rf_param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregressor__n_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m300\u001b[39m],\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregressor__max_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m],\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregressor__min_samples_split\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m],\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregressor__min_samples_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m]\n\u001b[1;32m     42\u001b[0m }\n\u001b[1;32m     44\u001b[0m rf_grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(rf_pipeline, rf_param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m rf_grid_search\u001b[38;5;241m.\u001b[39mfit(X_train_poly, y_train)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Evaluate Random Forest model\u001b[39;00m\n\u001b[1;32m     48\u001b[0m rf_best_model \u001b[38;5;241m=\u001b[39m rf_grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m         clone(base_estimator),\n\u001b[1;32m    824\u001b[0m         X,\n\u001b[1;32m    825\u001b[0m         y,\n\u001b[1;32m    826\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    827\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    828\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    829\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    830\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    831\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m    834\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the data\n",
    "file_path = '/Users/merryxmm7/Documents/GitHub/ML_Project_Fashion_Trends/datascience_salaries.csv'\n",
    "df_main = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df_main.drop(['salary', 'salary_currency', 'work_year'], axis=1, inplace=True)\n",
    "\n",
    "# Encode categorical variables with one-hot encoding\n",
    "df_main_encoded = pd.get_dummies(df_main, drop_first=True)\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = df_main_encoded.drop('salary_in_usd', axis=1)\n",
    "y = df_main_encoded['salary_in_usd']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Engineering\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "X_test_poly = poly_features.transform(X_test)\n",
    "\n",
    "# Model Selection and Hyperparameter Tuning (Random Forest)\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "rf_param_grid = {\n",
    "    'regressor__n_estimators': [100, 200, 300],\n",
    "    'regressor__max_depth': [None, 5, 10],\n",
    "    'regressor__min_samples_split': [2, 5, 10],\n",
    "    'regressor__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_grid_search = GridSearchCV(rf_pipeline, rf_param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "rf_grid_search.fit(X_train_poly, y_train)\n",
    "\n",
    "# Evaluate Random Forest model\n",
    "rf_best_model = rf_grid_search.best_estimator_\n",
    "y_pred_rf = rf_best_model.predict(X_test_poly)\n",
    "rf_r2 = r2_score(y_test, y_pred_rf)\n",
    "rf_mse = mean_squared_error(y_test, y_pred_rf)\n",
    "rf_mae = mean_absolute_error(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Random Forest Regression:\")\n",
    "print(f\"R2 Score: {rf_r2:.2f}\")\n",
    "print(f\"Mean Squared Error: {rf_mse:.2f}\")\n",
    "print(f\"Mean Absolute Error: {rf_mae:.2f}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830455c9-f93a-485b-9825-2ddf06a64871",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# the *: if we want to pass the value parameter, we have to pass it by name\n",
    "# So its going to be more obvious in the code when we are creating a leaf node\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None,*, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value=None\n",
    "        \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_sample_split=2, max_depth=100, n_features=None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features\n",
    "        self.root=None\n",
    "\n",
    "    def fit(self, X,y):\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1], self.n_features)\n",
    "\n",
    "        self.root = self._grow_treee(X,y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_feats = X.shape\n",
    "        n_labels = len(np.unique(y\n",
    "                                \n",
    "        #check the stopping criteria\n",
    "        if(depth>= self.max_depth or n_labels==1 or n_samples<self.min_samples_split):\n",
    "           leaf_value = self.most_common_label(y)\n",
    "           \n",
    "           return Node(value=leaf_value)\n",
    "\n",
    "        feat_idx = np.random.choice(n_feats, self.n_features, replace=False)\n",
    "                                 \n",
    "        #find the best split\n",
    "        best_feature, best_thresh  = self._best_split(X, y, feat_idx)\n",
    "        \n",
    "        #create child nodes\n",
    "    left_idxs, right_idxs = self._split(X[:,best_feature],best_thresh)\n",
    "    left = self._grow_tree(X[left_idxs,:], y[left_idxs], depth + 1)\n",
    "    \n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "\n",
    "    for feat_idx, in feat_idxs:\n",
    "        X_column = X[:, feat_idx]\n",
    "        thresholds = np.unique(X_column)\n",
    "        \n",
    "        for thr in thresholds:\n",
    "            #calculate the information gain\n",
    "            gain = self._information_gain(y, X_column, thr)\n",
    "\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                split_idx = feat_idx\n",
    "                split_threshold = thr\n",
    "\n",
    "    return split_idx, split_threshold\n",
    "\n",
    "    def _information_gain(self, y, X_column, threshold):\n",
    "        #parent entropy\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        #chreate children\n",
    "        left_idx, right_idx = self._split(X_column, threshold)\n",
    "\n",
    "    if len(left_idxs) == 0 or len(right_idxs) == :\n",
    "        return 0\n",
    "\n",
    "        #calculate the weighted  AVG. entropy of children\n",
    "    n = len(y)\n",
    "    n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "    e_l, e_r = self.entropy(y[left_idxs]), self.entropy(y[right_idxs])\n",
    "    child_entropy = (n_l/n)*e_l + (n_r/n)* e_r\n",
    "\n",
    "    #calculate the IG\n",
    "    information_gain = parent_entropy - child_entropy\n",
    "    return information_gain\n",
    "\n",
    "\n",
    "    def _split(self, X_column, split_threshold)\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).faltten\n",
    "        right_idxs= np.argwhere(X_column > split_thresh).faltten\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist / len(y)\n",
    "        return -np.sum([p*np.log(p) for p in ps if p>0])\n",
    "    \n",
    "    def most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        value = counter.most_common(1)[0][0]\n",
    "        return value\n",
    "                       \n",
    "\n",
    "    def predict():\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf0db98-c4e6-46f8-befd-441a8e7d7d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafcf404-ccca-4ad5-84c7-a5801fa4edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main[\"company_location\"].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83644478-6fc2-4357-9d1c-ada99fa0783b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
